{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 背景\n",
    "attention机制最早在NLP领域，在训练中增加一个逻辑计算针对权重的机制，提高对目标有关参数的权重，降低对目标无关的权重。2018年[CBMA](https://arxiv.org/abs/1807.06521)提出卷积网络中的注意力机制，包括一维的channel attention和二维的spatial attention两个部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\001\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from mxnet.gluon import data as gdata, nn\n",
    "from mxnet import contrib, image, nd,gluon,autograd, init\n",
    "import os,sys\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "ctx = mx.gpu()\n",
    "\n",
    "resize=(32,32)\n",
    "\n",
    "if not os.path.exists('output'):\n",
    "    os.makedirs('output')\n",
    "\n",
    "save_prefix = \"output/attention\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用FasionMNIST做测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#. trainset  60000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def load_mnist(batch_size, resize = None):\n",
    "    mnist_train = gdata.vision.FashionMNIST(train=True)\n",
    "    mnist_test = gdata.vision.FashionMNIST(train=False)\n",
    "\n",
    "    transformer_train, transformer_test = [],[]\n",
    "    if resize:\n",
    "        transformer_train.append(gdata.vision.transforms.Resize(resize))\n",
    "        transformer_test.append(gdata.vision.transforms.Resize(resize))\n",
    "    transformer_train.append(gdata.vision.transforms.RandomFlipLeftRight()) #没做flip，train acc 92% test acc 91%\n",
    "    transformer_train += [gdata.vision.transforms.ToTensor()]\n",
    "    transformer_train = gdata.vision.transforms.Compose(transformer_train)\n",
    "    \n",
    "    transformer_test += [gdata.vision.transforms.ToTensor()]\n",
    "    transformer_test = gdata.vision.transforms.Compose(transformer_test)\n",
    "    num_worker = 0 if sys.platform.startswith(\"win32\") else 2\n",
    "    train_iter = gdata.DataLoader(mnist_train.transform_first(transformer_train),batch_size,shuffle=True,\n",
    "                                  last_batch=\"rollover\",num_workers=num_worker)\n",
    "    test_iter = gdata.DataLoader(mnist_test.transform_first(transformer_test),batch_size,shuffle=False,\n",
    "                                 last_batch=\"rollover\",num_workers=num_worker)\n",
    "    return train_iter,test_iter,len(mnist_train)\n",
    "\n",
    "def get_class_names():\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return text_labels\n",
    "\n",
    "train_iter,valid_iter,num_trainset = load_mnist(batch_size,resize)\n",
    "num_classes = len(get_class_names())\n",
    "\n",
    "print('#. trainset ',num_trainset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义网络\n",
    "Q：如果conv_bn_relu()中是包含一个Conv-bn层，则训练中十分容易遇到NaN问题     \n",
    "Q: 两个不使用非线性激励函数的FC层，训练中容易得到NaN     \n",
    "   其中之一使用激励函数relu后，初始化采用normal(0.5)依然容易输出很大/很小的值，导致nan；但把normal(0.5)替换成xavier至少第一轮可以避免nan.\n",
    "   但如果此时conv_bn_relu()中卷积层减少到1时，依然容易导致NaN问题。尝试过交换BN层和Activation层次序，没有改善。   \n",
    "* 结论：这样一个浅网络，BN层未必起到规则化的作用?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_bn_relu(channel,kernel,stride,padding):\n",
    "    net = nn.Sequential()\n",
    "    net.add(\n",
    "        nn.Conv2D(channel, kernel, strides=stride, padding=padding),       \n",
    "        #nn.BatchNorm(),\n",
    "        nn.Activation(\"relu\"),\n",
    "        \n",
    "        nn.Conv2D(channel, kernel, strides=stride, padding=padding),\n",
    "        #nn.BatchNorm(),\n",
    "        nn.Activation(\"relu\"),\n",
    "    )\n",
    "    return net\n",
    "\n",
    "class BASENET(nn.Block):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BASENET,self).__init__()\n",
    "        self.stages = nn.Sequential()\n",
    "        self.stages.add(\n",
    "            conv_bn_relu(32,5,2,2),\n",
    "            nn.MaxPool2D(2),\n",
    "            conv_bn_relu(64,3,1,1),\n",
    "            nn.MaxPool2D(2),\n",
    "            conv_bn_relu(128,3,1,1),\n",
    "            nn.MaxPool2D(2),\n",
    "            nn.GlobalMaxPool2D(),\n",
    "            #nn.Dense(256), #BN层在浅网络中反而导致梯度爆炸？\n",
    "            #nn.Dense(256),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Dense(128),\n",
    "            nn.Dense(num_classes)\n",
    "        )\n",
    "        return\n",
    "    def forward(self, X):\n",
    "        Y = self.stages(X)\n",
    "\n",
    "        return Y\n",
    "    \n",
    "    \n",
    "class ATTNET(nn.Block): #通道AM\n",
    "    def __init__(self, num_classes):\n",
    "        super(ATTNET,self).__init__()\n",
    "        self.stageA, self.stageB, self.stageC = nn.Sequential(),nn.Sequential(),nn.Sequential()\n",
    "        self.stageD, self.stageE = nn.Sequential(),nn.Sequential()\n",
    "        self.stageA.add(\n",
    "            conv_bn_relu(32,5,2,2),\n",
    "            nn.MaxPool2D(2),\n",
    "            conv_bn_relu(64,3,1,1),\n",
    "            nn.MaxPool2D(2)\n",
    "        )\n",
    "        self.stageB.add(\n",
    "            nn.GlobalMaxPool2D(),\n",
    "            nn.Dense(32, activation=\"relu\"),\n",
    "            nn.Dense(64, activation=\"sigmoid\") #mxnet 支持sigmoid？\n",
    "        )\n",
    "        self.stageC.add(\n",
    "            nn.GlobalAvgPool2D(),\n",
    "            nn.Dense(32, activation=\"relu\"),\n",
    "            nn.Dense(64, activation=\"sigmoid\") #mxnet 支持sigmoid？\n",
    "        )\n",
    "        self.stageD.add(\n",
    "            nn.Dense(32, activation=\"relu\"),\n",
    "            nn.Dense(64, activation=\"sigmoid\") #mxnet 支持sigmoid？\n",
    "        )\n",
    "        self.stageE.add(\n",
    "            conv_bn_relu(128,3,1,1),\n",
    "            nn.MaxPool2D(2),\n",
    "            nn.GlobalMaxPool2D(),\n",
    "            nn.Dense(128),\n",
    "            #nn.Dropout(0.5),\n",
    "            #nn.Dense(128),\n",
    "            nn.Dense(num_classes)\n",
    "        )\n",
    "        return \n",
    "    \n",
    "    def forward(self,X):\n",
    "        Ya = self.stageA(X)\n",
    "        Yb = self.stageB(Ya) \n",
    "        Yc = self.stageC(Ya)\n",
    "        Ybc = self.stageD(Yb+Yc)\n",
    "        Ybc = nd.reshape(Ybc,(0,0,1,1)) #apply attention\n",
    "        Y = self.stageE(Ya*Ybc)\n",
    "        return Y\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 lr 0.2 42.24735713005066sec\n",
      "\ttrain loss 0.7450331449508667 ('train acc', 0.7178666666666667)\n",
      "\ttest loss 0.4256230890750885 ('test acc', 0.8471554487179487)\n",
      "\ttop valid acc 0.8471554487179487\n",
      "epoch 1 lr 0.2 41.316762924194336sec\n",
      "\ttrain loss 0.3948751389980316 ('train acc', 0.7859833333333334)\n",
      "\ttest loss 0.3850254714488983 ('test acc', 0.8533346645367412)\n",
      "\ttop valid acc 0.8533346645367412\n",
      "epoch 2 lr 0.2 41.236796140670776sec\n",
      "\ttrain loss 0.3428177535533905 ('train acc', 0.8147055555555556)\n",
      "\ttest loss 0.3435649275779724 ('test acc', 0.8734975961538461)\n",
      "\ttop valid acc 0.8734975961538461\n",
      "epoch 3 lr 0.2 40.847827196121216sec\n",
      "\ttrain loss 0.3159172236919403 ('train acc', 0.8319708333333333)\n",
      "\ttest loss 0.32022878527641296 ('test acc', 0.8790934504792333)\n",
      "\ttop valid acc 0.8790934504792333\n",
      "epoch 4 lr 0.2 40.86844205856323sec\n",
      "\ttrain loss 0.29972660541534424 ('train acc', 0.8431166666666666)\n",
      "\ttest loss 0.30280259251594543 ('test acc', 0.8848157051282052)\n",
      "\ttop valid acc 0.8848157051282052\n",
      "epoch 5 lr 0.2 40.601545572280884sec\n",
      "\ttrain loss 0.28619149327278137 ('train acc', 0.8513638888888889)\n",
      "\ttest loss 0.31351205706596375 ('test acc', 0.8813897763578274)\n",
      "epoch 6 lr 0.2 40.55769371986389sec\n",
      "\ttrain loss 0.27708205580711365 ('train acc', 0.8578380952380953)\n",
      "\ttest loss 0.2833424508571625 ('test acc', 0.8936298076923077)\n",
      "\ttop valid acc 0.8936298076923077\n",
      "epoch 7 lr 0.2 40.38131356239319sec\n",
      "\ttrain loss 0.2682448923587799 ('train acc', 0.8632395833333333)\n",
      "\ttest loss 0.29032373428344727 ('test acc', 0.8946685303514377)\n",
      "\ttop valid acc 0.8946685303514377\n",
      "epoch 8 lr 0.2 40.41727304458618sec\n",
      "\ttrain loss 0.2604883313179016 ('train acc', 0.8677425925925926)\n",
      "\ttest loss 0.27074506878852844 ('test acc', 0.8987379807692307)\n",
      "\ttop valid acc 0.8987379807692307\n",
      "epoch 9 lr 0.2 40.338781118392944sec\n",
      "\ttrain loss 0.25503936409950256 ('train acc', 0.8714816666666667)\n",
      "\ttest loss 0.30204832553863525 ('test acc', 0.8890774760383386)\n",
      "epoch 10 lr 0.2 40.71027731895447sec\n",
      "\ttrain loss 0.25048500299453735 ('train acc', 0.8747121212121212)\n",
      "\ttest loss 0.2697789669036865 ('test acc', 0.9018429487179487)\n",
      "\ttop valid acc 0.9018429487179487\n",
      "epoch 11 lr 0.020000000000000004 40.33118391036987sec\n",
      "\ttrain loss 0.1813114583492279 ('train acc', 0.8795027777777777)\n",
      "\ttest loss 0.23757685720920563 ('test acc', 0.9129392971246006)\n",
      "\ttop valid acc 0.9129392971246006\n",
      "epoch 12 lr 0.020000000000000004 40.585888624191284sec\n",
      "\ttrain loss 0.16449116170406342 ('train acc', 0.8840769230769231)\n",
      "\ttest loss 0.23618438839912415 ('test acc', 0.9151642628205128)\n",
      "\ttop valid acc 0.9151642628205128\n",
      "epoch 13 lr 0.020000000000000004 39.307457447052sec\n",
      "\ttrain loss 0.1560095250606537 ('train acc', 0.8882476190476191)\n",
      "\ttest loss 0.2308773696422577 ('test acc', 0.9170327476038339)\n",
      "\ttop valid acc 0.9170327476038339\n",
      "epoch 14 lr 0.020000000000000004 39.110111474990845sec\n",
      "\ttrain loss 0.1486705094575882 ('train acc', 0.8920455555555555)\n",
      "\ttest loss 0.23782801628112793 ('test acc', 0.9138621794871795)\n",
      "epoch 15 lr 0.020000000000000004 39.543022871017456sec\n",
      "\ttrain loss 0.14166676998138428 ('train acc', 0.8955052083333334)\n",
      "\ttest loss 0.23304162919521332 ('test acc', 0.919129392971246)\n",
      "\ttop valid acc 0.919129392971246\n",
      "epoch 16 lr 0.020000000000000004 38.876415729522705sec\n",
      "\ttrain loss 0.13537488877773285 ('train acc', 0.8987225490196078)\n",
      "\ttest loss 0.2340775579214096 ('test acc', 0.9165665064102564)\n",
      "epoch 17 lr 0.020000000000000004 39.20858907699585sec\n",
      "\ttrain loss 0.12944984436035156 ('train acc', 0.9017194444444444)\n",
      "\ttest loss 0.24011649191379547 ('test acc', 0.9157348242811502)\n",
      "epoch 18 lr 0.020000000000000004 39.35983228683472sec\n",
      "\ttrain loss 0.1234351247549057 ('train acc', 0.904530701754386)\n",
      "\ttest loss 0.24655161798000336 ('test acc', 0.9131610576923077)\n",
      "epoch 19 lr 0.020000000000000004 39.483999729156494sec\n",
      "\ttrain loss 0.11817505210638046 ('train acc', 0.9071675)\n",
      "\ttest loss 0.2411317676305771 ('test acc', 0.915535143769968)\n",
      "epoch 20 lr 0.020000000000000004 39.40156269073486sec\n",
      "\ttrain loss 0.11250897496938705 ('train acc', 0.9096611111111111)\n",
      "\ttest loss 0.24351535737514496 ('test acc', 0.9165665064102564)\n",
      "epoch 21 lr 0.0020000000000000005 39.33268332481384sec\n",
      "\ttrain loss 0.08985942602157593 ('train acc', 0.9123931818181819)\n",
      "\ttest loss 0.2483416199684143 ('test acc', 0.917132587859425)\n",
      "epoch 22 lr 0.0020000000000000005 39.063674211502075sec\n",
      "\ttrain loss 0.08528818935155869 ('train acc', 0.9149869565217391)\n",
      "\ttest loss 0.25194066762924194 ('test acc', 0.9162660256410257)\n",
      "epoch 23 lr 0.0020000000000000005 38.89340257644653sec\n",
      "\ttrain loss 0.08315233886241913 ('train acc', 0.9174076388888889)\n",
      "\ttest loss 0.25579795241355896 ('test acc', 0.9157348242811502)\n",
      "epoch 24 lr 0.0020000000000000005 39.37335443496704sec\n",
      "\ttrain loss 0.08137514442205429 ('train acc', 0.9196553333333334)\n",
      "\ttest loss 0.25764039158821106 ('test acc', 0.9165665064102564)\n",
      "epoch 25 lr 0.0020000000000000005 38.976540088653564sec\n",
      "\ttrain loss 0.07989657670259476 ('train acc', 0.92175)\n",
      "\ttest loss 0.2572595179080963 ('test acc', 0.9163338658146964)\n",
      "epoch 26 lr 0.0020000000000000005 38.98850154876709sec\n",
      "\ttrain loss 0.07841596007347107 ('train acc', 0.9237234567901235)\n",
      "\ttest loss 0.26184287667274475 ('test acc', 0.914863782051282)\n",
      "epoch 27 lr 0.0020000000000000005 40.31989574432373sec\n",
      "\ttrain loss 0.07694493234157562 ('train acc', 0.9255690476190476)\n",
      "\ttest loss 0.26068857312202454 ('test acc', 0.9161341853035144)\n",
      "epoch 28 lr 0.0020000000000000005 39.994038343429565sec\n",
      "\ttrain loss 0.07567667961120605 ('train acc', 0.9273114942528735)\n",
      "\ttest loss 0.2658448815345764 ('test acc', 0.9157652243589743)\n",
      "epoch 29 lr 0.0020000000000000005 38.89196562767029sec\n",
      "\ttrain loss 0.07427866011857986 ('train acc', 0.9289544444444444)\n",
      "\ttest loss 0.26632413268089294 ('test acc', 0.9157348242811502)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def test_net(net, valid_iter, ctx):\n",
    "    cls_loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    cls_acc = mx.metric.Accuracy(name=\"test acc\")\n",
    "    loss_sum = []\n",
    "    for batch in valid_iter:\n",
    "        X,Y = batch\n",
    "        out = X.as_in_context(ctx)\n",
    "        out = net(out)\n",
    "        out = out.as_in_context(mx.cpu())\n",
    "        cls_acc.update(Y,out)\n",
    "        loss = cls_loss(out, Y)\n",
    "        loss_sum.append( loss.mean().asscalar() )\n",
    "    print(\"\\ttest loss {} {}\".format( np.mean(loss_sum),cls_acc.get()))\n",
    "    return cls_acc.get_name_value()[0][1], np.mean(loss_sum)\n",
    "\n",
    "\n",
    "def train_net(net, train_iter, valid_iter, batch_size, trainer, ctx, num_epochs, lr_sch, save_prefix, train_log):\n",
    "    cls_loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    cls_acc = mx.metric.Accuracy(name=\"train acc\")\n",
    "    top_acc = 0\n",
    "    iter_num = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = [], 0\n",
    "        t0 = time.time()\n",
    "        trainer.set_learning_rate(lr_sch(epoch))\n",
    "        for batch in train_iter:\n",
    "            iter_num += 1\n",
    "            X,Y = batch\n",
    "            out = X.as_in_context(ctx)\n",
    "            with autograd.record(True):\n",
    "                out = net(out)\n",
    "                out = out.as_in_context(mx.cpu())\n",
    "                loss = cls_loss(out, Y)\n",
    "            loss.backward()\n",
    "            train_loss.append( loss.mean().asscalar() )\n",
    "            #print(loss.mean().asscalar())\n",
    "            trainer.step(batch_size)\n",
    "            cls_acc.update(Y,out)\n",
    "            nd.waitall()\n",
    "\n",
    "        print(\"epoch {} lr {} {}sec\".format(epoch,trainer.learning_rate, time.time() - t0))\n",
    "        train_loss,train_acc = np.mean(train_loss), cls_acc.get_name_value()[0][1]\n",
    "        print(\"\\ttrain loss {} {}\".format( np.mean(train_loss), cls_acc.get()))\n",
    "        acc,test_loss = test_net(net, valid_iter, ctx)\n",
    "        test_acc = acc\n",
    "        train_log.append((train_loss, train_acc, test_loss, test_acc))\n",
    "        \n",
    "        if top_acc < acc:\n",
    "            top_acc = acc\n",
    "            print('\\ttop valid acc {}'.format(acc))\n",
    "            if isinstance(net, mx.gluon.nn.HybridSequential) or isinstance(net, mx.gluon.nn.HybridBlock):\n",
    "                pf = '{}_{:.3f}.params'.format(save_prefix,top_acc)\n",
    "                net.export(pf,epoch)\n",
    "            else:\n",
    "                net_path = '{}top_acc_{}_{:.3f}.params'.format(save_prefix,epoch,top_acc)\n",
    "                net.save_parameters(net_path)\n",
    "       \n",
    "\n",
    "from mxnet import lr_scheduler\n",
    "num_epochs = 30\n",
    "logs = {0:[], 1:[]}\n",
    "for ind,net in enumerate([ ATTNET(num_classes),BASENET(num_classes)]):\n",
    "    net.initialize(init=init.Xavier(), ctx=ctx)\n",
    "    #net.initialize(init=init.Normal(0.5),ctx=ctx)\n",
    "    #trainer = gluon.Trainer(net.collect_params(), 'sgd',{'wd': 5e-4,\"momentum\":0.9,\"clip_gradient\":10})\n",
    "    #trainer = gluon.Trainer(net.collect_params(), 'sgd',{'wd': 5e-4,\"momentum\":0.9})\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd',{'wd': 5e-4})\n",
    "    lr_sch = lr_scheduler.FactorScheduler(step=10, factor=0.1)\n",
    "    lr_sch.base_lr = 0.2\n",
    "    train_net(net, train_iter, valid_iter, batch_size, trainer, ctx, num_epochs, lr_sch, save_prefix,logs[ind])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23985f7a748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "base_loss = [L[0] for L in logs[1]]\n",
    "base_acc = [L[1] for L in logs[1]]\n",
    "att_loss = [L[0] for L in logs[0]]\n",
    "att_acc = [L[1] for L in logs[0]]\n",
    "\n",
    "epochs = [k for k in range(num_epochs)]\n",
    "plt.plot(epochs, base_loss, color='r', label='base_loss')\n",
    "plt.plot(epochs, base_acc, color='c', label='base_acc')\n",
    "plt.plot(epochs, att_loss, color='b', label='att_loss')\n",
    "plt.plot(epochs, att_acc, color='g', label='att_acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
