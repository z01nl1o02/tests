{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triplet Loss损失函数在mnist上做相似度计算   \n",
    "triplet loss的核心包括三个部分    \n",
    "1. anchor/positive/negative   \n",
    "   代表三个输入图，尺寸相同，训练的目标是令anchor和positive距离最小化，同时anchor和negative距离最大化。以FaceRec为例，anchor和positive一般来自同一个人，而negative属于不同的另一个人。    \n",
    "2. shared models\n",
    "   通用的卷积模型，输入是单幅图像，输出是1维特征向量    \n",
    "3. loss \n",
    "   $$\n",
    "   L_i = [ (f(x_i^a) - f(x_i^p))^2 - (f(x_i^a) - f(x_i^n))^2 + \\alpha] \\\\\\\\\n",
    "   L = \\sum_i^N [max( L_i, 0)] \n",
    "   $$\n",
    "   其中$\\alpha$是marginal超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import os,sys\n",
    "import time\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import data as gdata\n",
    "from mxnet import gluon\n",
    "import random,cv2\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "ctx = mx.gpu()\n",
    "sample_size = (32,32)\n",
    "alpha = 1.0\n",
    "base_lr = 0.01 \n",
    "feat_dim = 2 \n",
    "smallset_num = batch_size * 2\n",
    "debug_flag = False\n",
    "\n",
    "epoch_num = 100\n",
    "\n",
    "if not os.path.exists('feat'):\n",
    "    os.makedirs('feat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feat_dim: 输出的特征图维度，设置成2时，训练阶段会保存测试机上类别分布图    \n",
    "smallset_num: 调试阶段，每一类只使用少量样本，设置成-1时，采用全部样本集进行训练    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set: 640, test set: 640 feat_set: 640\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class SingleMNIST(gluon.data.Dataset):\n",
    "    def __init__(self,fortrain,dataset_root=\"C:/dataset/mnist/\",resize=sample_size):\n",
    "        super(SingleMNIST,self).__init__()\n",
    "        self.data_pairs = []\n",
    "        labeldict = {}\n",
    "        self.total = 0\n",
    "        self.resize = resize\n",
    "        if fortrain:\n",
    "            ds_root = os.path.join(dataset_root,'train')\n",
    "        else:\n",
    "            ds_root = os.path.join(dataset_root,\"test\")\n",
    "        for rdir, pdirs, names in os.walk(ds_root):\n",
    "            for name in names:\n",
    "                basename,ext = os.path.splitext(name)\n",
    "                if ext != \".jpg\":\n",
    "                    continue\n",
    "                fullpath = os.path.join(rdir,name)\n",
    "                label = fullpath.split('\\\\')[-2]\n",
    "                label = int(label)\n",
    "                if smallset_num > 0 and ( label in labeldict.keys() ) and labeldict[label] >= smallset_num:\n",
    "                    continue\n",
    "                self.data_pairs.append((label,fullpath))\n",
    "                if label in labeldict:\n",
    "                    labeldict[label] += 1\n",
    "                else:\n",
    "                    labeldict[label] = 1\n",
    "                self.total += 1\n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_pairs)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        img = cv2.imread(self.data_pairs[idx][1],1)\n",
    "        label = self.data_pairs[idx][0]\n",
    "        \n",
    "        img = cv2.resize(img, self.resize)\n",
    "        \n",
    "\n",
    "        img = np.float32(img)/255\n",
    "        \n",
    "        img = np.transpose(img,(2,0,1))\n",
    "        \n",
    "        return (img,label)\n",
    "\n",
    "    \n",
    "class TripletMNIST(gluon.data.Dataset):\n",
    "    def __init__(self,fortrain,dataset_root=\"C:/dataset/mnist/\",resize=sample_size):\n",
    "        super(TripletMNIST,self).__init__()\n",
    "        self.data_pairs = {}\n",
    "        self.total = 0\n",
    "        self.resize = resize\n",
    "        if fortrain:\n",
    "            ds_root = os.path.join(dataset_root,'train')\n",
    "        else:\n",
    "            ds_root = os.path.join(dataset_root,\"test\")\n",
    "        for rdir, pdirs, names in os.walk(ds_root):\n",
    "            for name in names:\n",
    "                basename,ext = os.path.splitext(name)\n",
    "                if ext != \".jpg\":\n",
    "                    continue\n",
    "                fullpath = os.path.join(rdir,name)\n",
    "                label = fullpath.split('\\\\')[-2]\n",
    "                label = int(label)\n",
    "                if smallset_num > 0 and (label in self.data_pairs) and len(self.data_pairs[label]) >= smallset_num:\n",
    "                    continue \n",
    "\n",
    "                self.data_pairs.setdefault(label,[]).append(fullpath)\n",
    "                self.total += 1\n",
    "        self.class_num = len(self.data_pairs.keys())\n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        rds = np.random.randint(0,10000,size = 5)\n",
    "        rd_anchor_cls, rd_anchor_idx = rds[0], rds[1]\n",
    "        rd_anchor_cls = rd_anchor_cls % self.class_num\n",
    "        rd_anchor_idx = rd_anchor_idx % len(self.data_pairs[rd_anchor_cls])\n",
    "        \n",
    "        rd_pos_cls, rd_pos_idx = rd_anchor_cls, rds[2]\n",
    "        rd_pos_cls = rd_pos_cls % self.class_num\n",
    "        rd_pos_idx = rd_pos_idx % len(self.data_pairs[rd_pos_cls])\n",
    "        \n",
    "        rd_neg_cls, rd_neg_idx = rds[3], rds[4]\n",
    "        rd_neg_cls = rd_neg_cls % self.class_num\n",
    "        if rd_neg_cls == rd_pos_cls:\n",
    "            rd_neg_cls = (rd_neg_cls + 1)%self.class_num\n",
    "        rd_neg_idx = rd_neg_idx % len(self.data_pairs[rd_neg_cls])\n",
    "        \n",
    "        img_anchor = cv2.imread(self.data_pairs[rd_anchor_cls][rd_anchor_idx],1)\n",
    "        img_pos = cv2.imread(self.data_pairs[rd_pos_cls][rd_pos_idx],1)\n",
    "        img_neg = cv2.imread(self.data_pairs[rd_neg_cls][rd_neg_idx],1)\n",
    "        \n",
    "        img_anchor = cv2.resize(img_anchor, self.resize)\n",
    "        img_pos = cv2.resize(img_pos, self.resize)\n",
    "        img_neg = cv2.resize(img_neg, self.resize)\n",
    "        \n",
    "\n",
    "        img_anchor = np.float32(img_anchor)/255\n",
    "        img_pos = np.float32(img_pos)/255\n",
    "        img_neg = np.float32(img_neg)/255\n",
    "        \n",
    "        img_anchor = np.transpose(img_anchor,(2,0,1))\n",
    "        img_pos = np.transpose(img_pos,(2,0,1))\n",
    "        img_neg = np.transpose(img_neg,(2,0,1))\n",
    "        \n",
    "        return (img_anchor, img_pos, img_neg)\n",
    "\n",
    "\n",
    "\n",
    "if debug_flag:\n",
    "    ds = TripletMNIST(fortrain=True)\n",
    "    train_iter = gluon.data.DataLoader(ds,batch_size,shuffle=True,last_batch=\"rollover\")\n",
    "    for batch in train_iter:\n",
    "        anchor, pos, neg = batch\n",
    "        \n",
    "        #print(anchor.shape)\n",
    "        fig = plt.figure()\n",
    "        for k in range(8):\n",
    "            ax = fig.add_subplot(8,3,k*3+1)\n",
    "            img = anchor[k].asnumpy() * 255   \n",
    "            img = np.transpose(img,(1,2,0))\n",
    "            img = np.uint8(img)\n",
    "            ax.imshow(img)\n",
    "\n",
    "            \n",
    "            ax = fig.add_subplot(8,3,k*3+2)\n",
    "            img = pos[k].asnumpy() * 255\n",
    "            img = np.transpose(img,(1,2,0))\n",
    "            img = np.uint8(img)\n",
    "            ax.imshow(img)\n",
    "\n",
    "            ax = fig.add_subplot(8,3,k*3+3)\n",
    "            img = neg[k].asnumpy() * 255\n",
    "            img = np.transpose(img,(1,2,0))\n",
    "            img = np.uint8(img)\n",
    "            ax.imshow(img)\n",
    "\n",
    "        break\n",
    "    plt.show()\n",
    "train_ds = TripletMNIST(fortrain=True)\n",
    "train_iter = gluon.data.DataLoader(train_ds,batch_size,shuffle=True,last_batch=\"rollover\")\n",
    "test_ds = TripletMNIST(fortrain=False)\n",
    "test_iter = gluon.data.DataLoader(test_ds,batch_size,shuffle=False,last_batch=\"rollover\")\n",
    "\n",
    "feat_ds = SingleMNIST(fortrain=False)\n",
    "feat_iter = gluon.data.DataLoader(feat_ds, batch_size, shuffle=False, last_batch=\"rollover\")\n",
    "    \n",
    "print('train set: {}, test set: {} feat_set: {}'.format(len(train_ds), len(test_ds), len(feat_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHARED_NET(gluon.Block):\n",
    "    def __init__(self,root='./models'):\n",
    "        super(SHARED_NET,self).__init__()\n",
    "        backbone = gluon.model_zoo.vision.vgg11(pretrained=True,root = root)\n",
    "        self.net = gluon.nn.Sequential()\n",
    "        for layer in backbone.features[0:-4]:\n",
    "            self.net.add(layer)\n",
    "        self.net.add(gluon.nn.Dense(256),\n",
    "                    gluon.nn.Dense(feat_dim))\n",
    "        self.net[-1].initialize(mx.initializer.Xavier())\n",
    "        self.net[-2].initialize(mx.initializer.Xavier())\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "def get_net(num_class,root):\n",
    "    return SHARED_NET(num_class,root=root)\n",
    "\n",
    "\n",
    "if debug_flag:\n",
    "    fcn = SHARED_NET()\n",
    "    fcn.collect_params().reset_ctx(ctx)\n",
    "    x = mx.nd.random.uniform(0,1,[32,3] + list(sample_size),ctx=ctx)\n",
    "    print('input: ',x.shape)\n",
    "    y = fcn(x)\n",
    "    print('output: ',y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练阶段要每次要有$3xbatchSize$个图象经过网络，我们通过组成一个$3xbatchSize$的batch来实现，输出特征通过axis=0上分割获得anchor，pos和neg的特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import lr_scheduler\n",
    "import pdb\n",
    "import pickle\n",
    "\n",
    "def show_feat(epoch,net, feat_iter):\n",
    "    feats = []\n",
    "    for batch in feat_iter:\n",
    "        X,Y = batch\n",
    "        out = net(X.as_in_context(ctx)).asnumpy()\n",
    "        for y,f in zip(Y,out):\n",
    "            y = y.asnumpy()[0]\n",
    "            feats.append((y,f[0],f[1]))\n",
    "           # pdb.set_trace()\n",
    "    X = list(map(lambda d: d[1], feats))\n",
    "    Y = list(map(lambda d: d[2], feats))\n",
    "    #L = map(lambda d: d[0], feats)\n",
    "    #markers = 'o,<,+,1,2,3,4,8,s,h,*,D'.split(',')\n",
    "    cValue = [(1,0,0), (0,1,0), (0,0,1), \\\n",
    "              (0.5, 0, 0), (0,0.5,0), (0,0,0.5),\\\n",
    "              (1.0,1.0,0), (1.0,0,1.0), (0,1,1),\\\n",
    "              (0,0,0)] \n",
    "    L = []\n",
    "    for f in feats:\n",
    "        L.append(cValue[f[0]])\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    #pdb.set_trace()\n",
    "    ax.scatter(X,Y,c=L,marker='s')\n",
    "    plt.savefig('feat/%d.jpg'%epoch)\n",
    "    return\n",
    "        \n",
    "        \n",
    "def train_net(net, train_iter, valid_iter, feat_iter,batch_size, trainer, num_epochs, lr_sch, save_prefix):\n",
    "    iter_num = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        t0 = time.time()\n",
    "        train_loss = []\n",
    "        for batch in train_iter:\n",
    "            iter_num += 1\n",
    "            trainer.set_learning_rate(lr_sch(iter_num))\n",
    "            anchor, pos, neg = batch\n",
    "            #pdb.set_trace()\n",
    "            X = nd.concat(anchor, pos, neg, dim=0) #combine three inputs along 0-dim to create one batch\n",
    "            out = X.as_in_context(ctx)\n",
    "            #print(out.shape)\n",
    "            with mx.autograd.record(True):\n",
    "                out = net(out)\n",
    "                #out = out.as_in_context(mx.cpu())\n",
    "                out_anchor = out[0:batch_size]\n",
    "                out_pos = out[batch_size:batch_size*2]\n",
    "                out_neg = out[batch_size*2 : batch_size*3]\n",
    "                loss_anchor_pos = (out_anchor - out_pos)**2\n",
    "                loss_anchor_neg = (out_anchor - out_neg)**2\n",
    "                #print(loss_anchor_pos.max())\n",
    "                loss = loss_anchor_pos - loss_anchor_neg\n",
    "                loss = nd.relu(loss.sum(axis=1) + alpha).mean()\n",
    "            loss.backward()\n",
    "            train_loss.append( loss.asnumpy()[0] )\n",
    "            trainer.step(1)\n",
    "           # print(\"\\titer {} train loss {}\".format(iter_num,np.asarray(train_loss).mean()))\n",
    "            nd.waitall()\n",
    "        if (epoch % 10) == 0 and feat_dim == 2:\n",
    "            show_feat(epoch,net,feat_iter)\n",
    "        print(\"epoch {} lr {:>.5} loss {:>.5} cost {:>.3}sec\".format(epoch,trainer.learning_rate, \\\n",
    "                                                             np.asarray(train_loss).mean(),time.time() - t0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 lr 0.0099 loss 0.51676 cost 5.64sec\n",
      "epoch 1 lr 0.0098 loss 0.18289 cost 3.77sec\n",
      "epoch 2 lr 0.0097 loss 0.17965 cost 3.76sec\n",
      "epoch 3 lr 0.0096 loss 0.099185 cost 3.78sec\n",
      "epoch 4 lr 0.0095 loss 0.11342 cost 3.8sec\n",
      "epoch 5 lr 0.0094 loss 0.10642 cost 3.78sec\n",
      "epoch 6 lr 0.0093 loss 0.099273 cost 3.76sec\n",
      "epoch 7 lr 0.0092 loss 0.052797 cost 3.81sec\n",
      "epoch 8 lr 0.0091 loss 0.0427 cost 3.77sec\n",
      "epoch 9 lr 0.009 loss 0.033315 cost 3.77sec\n",
      "epoch 10 lr 0.0089 loss 0.039496 cost 4.7sec\n"
     ]
    }
   ],
   "source": [
    "net = SHARED_NET()\n",
    "net.collect_params().reset_ctx(ctx)\n",
    "wd = 0.0005\n",
    "trainer = gluon.Trainer(net.collect_params(),optimizer=\"sgd\",optimizer_params={\"wd\":wd})\n",
    "\n",
    "lr_plan = lr_scheduler.PolyScheduler(max_update= epoch_num * len(train_ds) // batch_size,base_lr=base_lr, pwr=1)\n",
    "train_net(net, train_iter, test_iter, feat_iter, batch_size,trainer, epoch_num, lr_plan,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
