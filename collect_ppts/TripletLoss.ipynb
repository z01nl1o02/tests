{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triplet Loss损失函数在mnist上做相似度计算   \n",
    "triplet loss的核心包括三个部分    \n",
    "1. anchor/positive/negative   \n",
    "   代表三个输入图，尺寸相同，训练的目标是令anchor和positive距离最小化，同时anchor和negative距离最大化。以FaceRec为例，anchor和positive一般来自同一个人，而negative属于不同的另一个人。    \n",
    "2. shared models\n",
    "   通用的卷积模型，输入是单幅图像，输出是1维特征向量    \n",
    "3. loss \n",
    "   $$\n",
    "   L_i = [ (f(x_i^a) - f(x_i^p))^2 - (f(x_i^a) - f(x_i^n))^2 + \\alpha] \\\\\\\\\n",
    "   L = \\sum_i^N [max( L_i, 0)] \n",
    "   $$\n",
    "   其中$\\alpha$是marginal超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import os,sys\n",
    "import time\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import data as gdata\n",
    "from mxnet import gluon\n",
    "import random,cv2\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "ctx = mx.gpu()\n",
    "sample_size = (32,32)\n",
    "alpha = 1.0\n",
    "base_lr = 0.01\n",
    "feat_dim = 2\n",
    "smallset_num = batch_size * 2\n",
    "debug_flag = False\n",
    "\n",
    "if not os.path.exists('feat'):\n",
    "    os.makedirs('feat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set: 640, test set: 640 feat_set: 640\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class SingleMNIST(gluon.data.Dataset):\n",
    "    def __init__(self,fortrain,dataset_root=\"C:/dataset/mnist/\",resize=sample_size):\n",
    "        super(SingleMNIST,self).__init__()\n",
    "        self.data_pairs = []\n",
    "        labeldict = {}\n",
    "        self.total = 0\n",
    "        self.resize = resize\n",
    "        if fortrain:\n",
    "            ds_root = os.path.join(dataset_root,'train')\n",
    "        else:\n",
    "            ds_root = os.path.join(dataset_root,\"test\")\n",
    "        for rdir, pdirs, names in os.walk(ds_root):\n",
    "            for name in names:\n",
    "                basename,ext = os.path.splitext(name)\n",
    "                if ext != \".jpg\":\n",
    "                    continue\n",
    "                fullpath = os.path.join(rdir,name)\n",
    "                label = fullpath.split('\\\\')[-2]\n",
    "                label = int(label)\n",
    "                if smallset_num > 0 and ( label in labeldict.keys() ) and labeldict[label] >= smallset_num:\n",
    "                    continue\n",
    "                self.data_pairs.append((label,fullpath))\n",
    "                if label in labeldict:\n",
    "                    labeldict[label] += 1\n",
    "                else:\n",
    "                    labeldict[label] = 1\n",
    "                self.total += 1\n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_pairs)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        img = cv2.imread(self.data_pairs[idx][1],1)\n",
    "        label = self.data_pairs[idx][0]\n",
    "        \n",
    "        img = cv2.resize(img, self.resize)\n",
    "        \n",
    "\n",
    "        img = np.float32(img)/255\n",
    "        \n",
    "        img = np.transpose(img,(2,0,1))\n",
    "        \n",
    "        return (img,label)\n",
    "\n",
    "    \n",
    "class TripletMNIST(gluon.data.Dataset):\n",
    "    def __init__(self,fortrain,dataset_root=\"C:/dataset/mnist/\",resize=sample_size):\n",
    "        super(TripletMNIST,self).__init__()\n",
    "        self.data_pairs = {}\n",
    "        self.total = 0\n",
    "        self.resize = resize\n",
    "        if fortrain:\n",
    "            ds_root = os.path.join(dataset_root,'train')\n",
    "        else:\n",
    "            ds_root = os.path.join(dataset_root,\"test\")\n",
    "        for rdir, pdirs, names in os.walk(ds_root):\n",
    "            for name in names:\n",
    "                basename,ext = os.path.splitext(name)\n",
    "                if ext != \".jpg\":\n",
    "                    continue\n",
    "                fullpath = os.path.join(rdir,name)\n",
    "                label = fullpath.split('\\\\')[-2]\n",
    "                label = int(label)\n",
    "                if smallset_num > 0 and (label in self.data_pairs) and len(self.data_pairs[label]) >= smallset_num:\n",
    "                    continue \n",
    "\n",
    "                self.data_pairs.setdefault(label,[]).append(fullpath)\n",
    "                self.total += 1\n",
    "        self.class_num = len(self.data_pairs.keys())\n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        rds = np.random.randint(0,10000,size = 5)\n",
    "        rd_anchor_cls, rd_anchor_idx = rds[0], rds[1]\n",
    "        rd_anchor_cls = rd_anchor_cls % self.class_num\n",
    "        rd_anchor_idx = rd_anchor_idx % len(self.data_pairs[rd_anchor_cls])\n",
    "        \n",
    "        rd_pos_cls, rd_pos_idx = rd_anchor_cls, rds[2]\n",
    "        rd_pos_cls = rd_pos_cls % self.class_num\n",
    "        rd_pos_idx = rd_pos_idx % len(self.data_pairs[rd_pos_cls])\n",
    "        \n",
    "        rd_neg_cls, rd_neg_idx = rds[3], rds[4]\n",
    "        rd_neg_cls = rd_neg_cls % self.class_num\n",
    "        if rd_neg_cls == rd_pos_cls:\n",
    "            rd_neg_cls = (rd_neg_cls + 1)%self.class_num\n",
    "        rd_neg_idx = rd_neg_idx % len(self.data_pairs[rd_neg_cls])\n",
    "        \n",
    "        img_anchor = cv2.imread(self.data_pairs[rd_anchor_cls][rd_anchor_idx],1)\n",
    "        img_pos = cv2.imread(self.data_pairs[rd_pos_cls][rd_pos_idx],1)\n",
    "        img_neg = cv2.imread(self.data_pairs[rd_neg_cls][rd_neg_idx],1)\n",
    "        \n",
    "        img_anchor = cv2.resize(img_anchor, self.resize)\n",
    "        img_pos = cv2.resize(img_pos, self.resize)\n",
    "        img_neg = cv2.resize(img_neg, self.resize)\n",
    "        \n",
    "\n",
    "        img_anchor = np.float32(img_anchor)/255\n",
    "        img_pos = np.float32(img_pos)/255\n",
    "        img_neg = np.float32(img_neg)/255\n",
    "        \n",
    "        img_anchor = np.transpose(img_anchor,(2,0,1))\n",
    "        img_pos = np.transpose(img_pos,(2,0,1))\n",
    "        img_neg = np.transpose(img_neg,(2,0,1))\n",
    "        \n",
    "        return (img_anchor, img_pos, img_neg)\n",
    "\n",
    "\n",
    "\n",
    "if debug_flag:\n",
    "    ds = TripletMNIST(fortrain=True)\n",
    "    train_iter = gluon.data.DataLoader(ds,batch_size,shuffle=True,last_batch=\"rollover\")\n",
    "    for batch in train_iter:\n",
    "        anchor, pos, neg = batch\n",
    "        \n",
    "        #print(anchor.shape)\n",
    "        fig = plt.figure()\n",
    "        for k in range(8):\n",
    "            ax = fig.add_subplot(8,3,k*3+1)\n",
    "            img = anchor[k].asnumpy() * 255   \n",
    "            img = np.transpose(img,(1,2,0))\n",
    "            img = np.uint8(img)\n",
    "            ax.imshow(img)\n",
    "\n",
    "            \n",
    "            ax = fig.add_subplot(8,3,k*3+2)\n",
    "            img = pos[k].asnumpy() * 255\n",
    "            img = np.transpose(img,(1,2,0))\n",
    "            img = np.uint8(img)\n",
    "            ax.imshow(img)\n",
    "\n",
    "            ax = fig.add_subplot(8,3,k*3+3)\n",
    "            img = neg[k].asnumpy() * 255\n",
    "            img = np.transpose(img,(1,2,0))\n",
    "            img = np.uint8(img)\n",
    "            ax.imshow(img)\n",
    "\n",
    "        break\n",
    "    plt.show()\n",
    "train_ds = TripletMNIST(fortrain=True)\n",
    "train_iter = gluon.data.DataLoader(train_ds,batch_size,shuffle=True,last_batch=\"rollover\")\n",
    "test_ds = TripletMNIST(fortrain=False)\n",
    "test_iter = gluon.data.DataLoader(test_ds,batch_size,shuffle=False,last_batch=\"rollover\")\n",
    "\n",
    "feat_ds = SingleMNIST(fortrain=False)\n",
    "feat_iter = gluon.data.DataLoader(feat_ds, batch_size, shuffle=False, last_batch=\"rollover\")\n",
    "    \n",
    "print('train set: {}, test set: {} feat_set: {}'.format(len(train_ds), len(test_ds), len(feat_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHARED_NET(gluon.Block):\n",
    "    def __init__(self,root='./models'):\n",
    "        super(SHARED_NET,self).__init__()\n",
    "        backbone = gluon.model_zoo.vision.vgg11(pretrained=True,root = root)\n",
    "        self.net = gluon.nn.Sequential()\n",
    "        for layer in backbone.features[0:-4]:\n",
    "            self.net.add(layer)\n",
    "        self.net.add(gluon.nn.Dense(256,activation=\"sigmoid\"),\n",
    "                    gluon.nn.Dense(feat_dim,activation=\"sigmoid\"))\n",
    "        self.net[-1].initialize(mx.initializer.Xavier())\n",
    "        self.net[-2].initialize(mx.initializer.Xavier())\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "def get_net(num_class,root):\n",
    "    return SHARED_NET(num_class,root=root)\n",
    "\n",
    "\n",
    "if debug_flag:\n",
    "    fcn = SHARED_NET()\n",
    "    fcn.collect_params().reset_ctx(ctx)\n",
    "    x = mx.nd.random.uniform(0,1,[32,3] + list(sample_size),ctx=ctx)\n",
    "    print('input: ',x.shape)\n",
    "    y = fcn(x)\n",
    "    print('output: ',y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练阶段要每次要有$3xbatchSize$个图象经过网络，我们通过组成一个$3xbatchSize$的batch来实现，输出特征通过axis=0上分割获得anchor，pos和neg的特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import lr_scheduler\n",
    "import pdb\n",
    "import pickle\n",
    "\n",
    "def show_feat(epoch,net, feat_iter):\n",
    "    feats = []\n",
    "    for batch in feat_iter:\n",
    "        X,Y = batch\n",
    "        out = net(X.as_in_context(ctx)).asnumpy()\n",
    "        for y,f in zip(Y,out):\n",
    "            y = y.asnumpy()[0]\n",
    "            feats.append((y,f[0],f[1]))\n",
    "           # pdb.set_trace()\n",
    "    X = list(map(lambda d: d[1], feats))\n",
    "    Y = list(map(lambda d: d[2], feats))\n",
    "    #L = map(lambda d: d[0], feats)\n",
    "    #markers = 'o,<,+,1,2,3,4,8,s,h,*,D'.split(',')\n",
    "    L = []\n",
    "    for f in feats:\n",
    "        L.append(f[0] / 10.0)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    #pdb.set_trace()\n",
    "    ax.scatter(X,Y,c=L, cmap = plt.get_cmap('autumn'))\n",
    "    plt.savefig('feat/%d.jpg'%epoch)\n",
    "    return\n",
    "        \n",
    "        \n",
    "def train_net(net, train_iter, valid_iter, feat_iter,batch_size, trainer, num_epochs, lr_sch, save_prefix):\n",
    "    iter_num = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        t0 = time.time()\n",
    "        train_loss = []\n",
    "        for batch in train_iter:\n",
    "            iter_num += 1\n",
    "            trainer.set_learning_rate(lr_sch(iter_num))\n",
    "            anchor, pos, neg = batch\n",
    "            #pdb.set_trace()\n",
    "            X = nd.concat(anchor, pos, neg, dim=0) #combine three inputs along 0-dim to create one batch\n",
    "            out = X.as_in_context(ctx)\n",
    "            #print(out.shape)\n",
    "            with mx.autograd.record(True):\n",
    "                out = net(out)\n",
    "                #out = out.as_in_context(mx.cpu())\n",
    "                out_anchor = out[0:batch_size]\n",
    "                out_pos = out[batch_size:batch_size*2]\n",
    "                out_neg = out[batch_size*2 : batch_size*3]\n",
    "                loss_anchor_pos = (out_anchor - out_pos)**2\n",
    "                loss_anchor_neg = (out_anchor - out_neg)**2\n",
    "                #print(loss_anchor_pos.max())\n",
    "                loss = loss_anchor_pos - loss_anchor_neg\n",
    "                loss = nd.relu(loss.sum(axis=1) + alpha).mean()\n",
    "            loss.backward()\n",
    "            train_loss.append( loss.asnumpy()[0] )\n",
    "            trainer.step(1)\n",
    "           # print(\"\\titer {} train loss {}\".format(iter_num,np.asarray(train_loss).mean()))\n",
    "            nd.waitall()\n",
    "        if (epoch % 10) == 0:\n",
    "            show_feat(epoch,net,feat_iter)\n",
    "        print(\"epoch {} lr {:>.5} loss {:>.5} cost {:>.3}sec\".format(epoch,trainer.learning_rate, \\\n",
    "                                                             np.asarray(train_loss).mean(),time.time() - t0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 lr 0.0099 loss 0.99794 cost 16.4sec\n",
      "epoch 1 lr 0.0098 loss 0.99678 cost 3.8sec\n",
      "epoch 2 lr 0.0097 loss 0.99444 cost 3.73sec\n",
      "epoch 3 lr 0.0096 loss 0.98715 cost 3.89sec\n",
      "epoch 4 lr 0.0095 loss 0.95797 cost 3.8sec\n",
      "epoch 5 lr 0.0094 loss 0.86236 cost 3.74sec\n",
      "epoch 6 lr 0.0093 loss 0.66619 cost 3.72sec\n",
      "epoch 7 lr 0.0092 loss 0.56043 cost 3.7sec\n",
      "epoch 8 lr 0.0091 loss 0.57582 cost 3.73sec\n",
      "epoch 9 lr 0.009 loss 0.55632 cost 3.89sec\n",
      "epoch 10 lr 0.0089 loss 0.54346 cost 4.73sec\n",
      "epoch 11 lr 0.0088 loss 0.56027 cost 3.76sec\n",
      "epoch 12 lr 0.0087 loss 0.51526 cost 3.73sec\n",
      "epoch 13 lr 0.0086 loss 0.47391 cost 3.73sec\n",
      "epoch 14 lr 0.0085 loss 0.46273 cost 3.8sec\n",
      "epoch 15 lr 0.0084 loss 0.48076 cost 3.75sec\n",
      "epoch 16 lr 0.0083 loss 0.50109 cost 3.78sec\n",
      "epoch 17 lr 0.0082 loss 0.46132 cost 3.77sec\n",
      "epoch 18 lr 0.0081 loss 0.43796 cost 3.75sec\n",
      "epoch 19 lr 0.008 loss 0.45922 cost 3.69sec\n",
      "epoch 20 lr 0.0079 loss 0.43995 cost 4.66sec\n",
      "epoch 21 lr 0.0078 loss 0.43756 cost 3.85sec\n",
      "epoch 22 lr 0.0077 loss 0.44729 cost 3.77sec\n",
      "epoch 23 lr 0.0076 loss 0.43857 cost 3.97sec\n",
      "epoch 24 lr 0.0075 loss 0.40084 cost 3.87sec\n",
      "epoch 25 lr 0.0074 loss 0.38127 cost 3.8sec\n",
      "epoch 26 lr 0.0073 loss 0.42528 cost 3.86sec\n",
      "epoch 27 lr 0.0072 loss 0.42945 cost 3.81sec\n",
      "epoch 28 lr 0.0071 loss 0.38567 cost 3.7sec\n",
      "epoch 29 lr 0.007 loss 0.41009 cost 3.72sec\n",
      "epoch 30 lr 0.0069 loss 0.37492 cost 4.73sec\n",
      "epoch 31 lr 0.0068 loss 0.41823 cost 3.7sec\n",
      "epoch 32 lr 0.0067 loss 0.41636 cost 3.7sec\n",
      "epoch 33 lr 0.0066 loss 0.42208 cost 3.84sec\n",
      "epoch 34 lr 0.0065 loss 0.41601 cost 3.77sec\n",
      "epoch 35 lr 0.0064 loss 0.38929 cost 3.81sec\n",
      "epoch 36 lr 0.0063 loss 0.38817 cost 3.78sec\n",
      "epoch 37 lr 0.0062 loss 0.40736 cost 3.72sec\n",
      "epoch 38 lr 0.0061 loss 0.36566 cost 3.7sec\n",
      "epoch 39 lr 0.006 loss 0.39473 cost 3.74sec\n",
      "epoch 40 lr 0.0059 loss 0.37129 cost 4.73sec\n",
      "epoch 41 lr 0.0058 loss 0.42921 cost 3.95sec\n",
      "epoch 42 lr 0.0057 loss 0.41424 cost 3.81sec\n",
      "epoch 43 lr 0.0056 loss 0.39844 cost 3.81sec\n",
      "epoch 44 lr 0.0055 loss 0.38526 cost 3.8sec\n",
      "epoch 45 lr 0.0054 loss 0.38455 cost 3.86sec\n",
      "epoch 46 lr 0.0053 loss 0.38576 cost 3.8sec\n",
      "epoch 47 lr 0.0052 loss 0.38317 cost 3.75sec\n",
      "epoch 48 lr 0.0051 loss 0.37804 cost 3.81sec\n",
      "epoch 49 lr 0.005 loss 0.4307 cost 3.89sec\n",
      "epoch 50 lr 0.0049 loss 0.40576 cost 4.89sec\n",
      "epoch 51 lr 0.0048 loss 0.40818 cost 3.87sec\n",
      "epoch 52 lr 0.0047 loss 0.39533 cost 3.94sec\n",
      "epoch 53 lr 0.0046 loss 0.36716 cost 3.87sec\n",
      "epoch 54 lr 0.0045 loss 0.36726 cost 3.76sec\n",
      "epoch 55 lr 0.0044 loss 0.42306 cost 3.82sec\n",
      "epoch 56 lr 0.0043 loss 0.35328 cost 3.7sec\n",
      "epoch 57 lr 0.0042 loss 0.4059 cost 3.7sec\n",
      "epoch 58 lr 0.0041 loss 0.39816 cost 3.7sec\n",
      "epoch 59 lr 0.004 loss 0.40635 cost 3.7sec\n",
      "epoch 60 lr 0.0039 loss 0.36654 cost 4.73sec\n",
      "epoch 61 lr 0.0038 loss 0.40374 cost 3.72sec\n",
      "epoch 62 lr 0.0037 loss 0.38204 cost 3.69sec\n",
      "epoch 63 lr 0.0036 loss 0.39583 cost 3.72sec\n",
      "epoch 64 lr 0.0035 loss 0.38068 cost 3.77sec\n",
      "epoch 65 lr 0.0034 loss 0.39228 cost 3.88sec\n",
      "epoch 66 lr 0.0033 loss 0.40033 cost 3.73sec\n",
      "epoch 67 lr 0.0032 loss 0.37849 cost 3.69sec\n",
      "epoch 68 lr 0.0031 loss 0.37991 cost 3.72sec\n",
      "epoch 69 lr 0.003 loss 0.37778 cost 3.72sec\n",
      "epoch 70 lr 0.0029 loss 0.39305 cost 4.7sec\n",
      "epoch 71 lr 0.0028 loss 0.39767 cost 3.78sec\n",
      "epoch 72 lr 0.0027 loss 0.37036 cost 3.73sec\n",
      "epoch 73 lr 0.0026 loss 0.37491 cost 3.97sec\n"
     ]
    }
   ],
   "source": [
    "net = SHARED_NET()\n",
    "net.collect_params().reset_ctx(ctx)\n",
    "wd = 0.0005\n",
    "trainer = gluon.Trainer(net.collect_params(),optimizer=\"sgd\",optimizer_params={\"wd\":wd})\n",
    "epoch_num = 100\n",
    "lr_plan = lr_scheduler.PolyScheduler(max_update= epoch_num * len(train_ds) // batch_size,base_lr=base_lr, pwr=1)\n",
    "train_net(net, train_iter, test_iter, feat_iter, batch_size,trainer, epoch_num, lr_plan,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
