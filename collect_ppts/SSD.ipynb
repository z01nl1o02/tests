{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[gluon](http://zh.gluon.ai/chapter_computer-vision/ssd.html)   [gluon](http://zh.gluon.ai/chapter_computer-vision/anchor.html)    \n",
    "这是一个没有预训练模型的SSD实现，也是因为没有预训练模型，只能用来处理一些比较简单的数据集。     \n",
    "\n",
    "如下是SSD的框架示意图   \n",
    "![SSD](images/ssd.svg)   \n",
    "\n",
    "输入图像经过一个backbone网络提取特征，然后进行若干次下采样，每个下采样层都附带一个边框预测和类别预测模块。注意和YOLOv3不同，SSD是基于backbone的输出特征图做多次下采样，这也导致SSD对小目标检测效果不好。    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义一个简单的Backbone网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\001\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from mxnet import contrib, image, nd,gluon,autograd, init\n",
    "from mxnet.gluon import loss as gloss, nn\n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#这个下采样层既用在backbone中，也用在后续的检测模块中\n",
    "def down_sample_blk(num_channels):\n",
    "    blk = nn.Sequential()\n",
    "    for _ in range(2):\n",
    "        blk.add(nn.Conv2D(num_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm(in_channels=num_channels),\n",
    "                nn.Activation('relu'))\n",
    "    blk.add(nn.MaxPool2D(2))\n",
    "    return blk\n",
    "\n",
    "#backbone\n",
    "def base_net():\n",
    "    blk = nn.Sequential()\n",
    "    for num_filters in [16, 32, 64]:\n",
    "        blk.add(down_sample_blk(num_filters))\n",
    "    return blk\n",
    "\n",
    "#完整的特征提取模块\n",
    "#0： backbone\n",
    "#1,2,3： 三个下采样层\n",
    "#4：一个global max pool层\n",
    "def get_blk(i):\n",
    "    if i == 0:\n",
    "        blk = base_net()\n",
    "    elif i == 4:\n",
    "        blk = nn.GlobalMaxPool2D()\n",
    "    else:\n",
    "        blk = down_sample_blk(128)\n",
    "    return blk\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义类别预测层和边框预测层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#类别预测层，这里用一个卷积层实现\n",
    "#其输入输出层的w/h必须一致\n",
    "#输出通道数C等于(anchor个数)x(类别数+1)，这里类别不包括背景，+1就是背景类\n",
    "def cls_predictor(num_anchors, num_classes):\n",
    "    return nn.Conv2D(num_anchors * (num_classes + 1), kernel_size=3,\n",
    "                     padding=1)\n",
    "\n",
    "\n",
    "#边框预测层，也用一个卷积层实现\n",
    "#输入输出的w/h必须一致\n",
    "#输出通道数C等于4x(anchor个数)\n",
    "def bbox_predictor(num_anchors):\n",
    "    return nn.Conv2D(num_anchors * 4, kernel_size=3, padding=1)\n",
    "\n",
    "\n",
    "#不同层预测出来的类别和边框的尺寸是不一样的，下面定义了一种合并方式\n",
    "#每一层输出的是shape是(batch,C,H,W),不同层的输出，只有batch是一致的，其他三个值都不一样\n",
    "#下面的函数把(batch,C,H,W)转换成(batch,HxWxC)\n",
    "#最后在dim=1上连接\n",
    "#注意mx.nd.flatten()的功能和numpy.flatten()不同，mx.nd.flatten()会保留维度0，只合并后面的维度\n",
    "def flatten_pred(pred):\n",
    "    return pred.transpose((0, 2, 3, 1)).flatten()\n",
    "\n",
    "def concat_preds(preds):\n",
    "    return nd.concat(*[flatten_pred(p) for p in preds], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义每个层如何预测 (SSD的重点）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blk_forward(X, blk, size, ratio, cls_predictor, bbox_predictor):\n",
    "    Y = blk(X) #提取特征\n",
    "    anchors = contrib.ndarray.MultiBoxPrior(Y, sizes=size, ratios=ratio) #获得anchor\n",
    "    cls_preds = cls_predictor(Y) #预测类别 （这不是上面定义的函数，而是其具体实现，即一个卷积层）\n",
    "    bbox_preds = bbox_predictor(Y) #预测边界框 （这不是上面定义的函数，而是其具体实现，即一个卷积层）\n",
    "    return (Y, anchors, cls_preds, bbox_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义SSD模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output anchors: (1, 5444, 4)\n",
      "output class preds: (32, 5444, 2)\n",
      "output bbox preds: (32, 21776)\n"
     ]
    }
   ],
   "source": [
    "class TinySSD(nn.Block):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super(TinySSD, self).__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.sizes = [[0.2, 0.272], [0.37, 0.447], [0.54, 0.619], [0.71, 0.79],[0.88, 0.961]]\n",
    "        self.ratios = [[1, 2, 0.5]] * 5\n",
    "        self.num_anchors = len(self.sizes[0]) + len(self.ratios[0]) - 1\n",
    "\n",
    "        self.stage_0, self.stage_1, self.stage_2, self.stage_3, self.stage_4 = nn.Sequential(),nn.Sequential(),nn.Sequential(),nn.Sequential(),nn.Sequential()\n",
    "        \n",
    "        self.stage_0.add(get_blk(0), cls_predictor(self.num_anchors, self.num_classes), bbox_predictor(self.num_anchors)) #backbone\n",
    "        self.stage_1.add(get_blk(1), cls_predictor(self.num_anchors, self.num_classes), bbox_predictor(self.num_anchors)) #第1个预测层\n",
    "        self.stage_2.add(get_blk(2), cls_predictor(self.num_anchors, self.num_classes), bbox_predictor(self.num_anchors)) #第2个预测层\n",
    "        self.stage_3.add(get_blk(3), cls_predictor(self.num_anchors, self.num_classes), bbox_predictor(self.num_anchors)) #第3个预测层\n",
    "        self.stage_4.add(get_blk(4), cls_predictor(self.num_anchors, self.num_classes), bbox_predictor(self.num_anchors)) #第4个预测层 \n",
    "        return\n",
    "    def forward(self, X):\n",
    "        anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5\n",
    "        X,anchors[0], cls_preds[0], bbox_preds[0] = blk_forward(X, self.stage_0[0], self.sizes[0], self.ratios[0], self.stage_0[1], self.stage_0[2])\n",
    "        X,anchors[1], cls_preds[1], bbox_preds[1] = blk_forward(X, self.stage_1[0], self.sizes[1], self.ratios[1], self.stage_1[1], self.stage_1[2])\n",
    "        X,anchors[2], cls_preds[2], bbox_preds[2] = blk_forward(X, self.stage_2[0], self.sizes[2], self.ratios[2], self.stage_2[1], self.stage_2[2])\n",
    "        X,anchors[3], cls_preds[3], bbox_preds[3] = blk_forward(X, self.stage_3[0], self.sizes[3], self.ratios[3], self.stage_3[1], self.stage_3[2])\n",
    "        X,anchors[4], cls_preds[4], bbox_preds[4] = blk_forward(X, self.stage_4[0], self.sizes[4], self.ratios[4], self.stage_4[1], self.stage_4[2])\n",
    "        # reshape函数中的0表示保持批量大小不变\n",
    "        return (nd.concat(*anchors, dim=1),\n",
    "                concat_preds(cls_preds).reshape(\n",
    "                    (0, -1, self.num_classes + 1)), concat_preds(bbox_preds))\n",
    "    \n",
    "    \n",
    "    \n",
    "net = TinySSD(num_classes=1)\n",
    "net.initialize()\n",
    "X = nd.zeros((32, 3, 256, 256))\n",
    "anchors, cls_preds, bbox_preds = net(X)\n",
    "\n",
    "print('output anchors:', anchors.shape)\n",
    "print('output class preds:', cls_preds.shape)\n",
    "print('output bbox preds:', bbox_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 皮卡丘数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon, image\n",
    "from mxnet.gluon import utils as gutils\n",
    "import os\n",
    "\n",
    "def _download_pikachu(data_dir):\n",
    "    root_url = ('https://apache-mxnet.s3-accelerate.amazonaws.com/'\n",
    "                'gluon/dataset/pikachu/')\n",
    "    dataset = {'train.rec': 'e6bcb6ffba1ac04ff8a9b1115e650af56ee969c8',\n",
    "               'train.idx': 'dcf7318b2602c06428b9988470c731621716c393',\n",
    "               'val.rec': 'd6c33f799b4d058e82f2cb5bd9a976f69d72d520'}\n",
    "    for k, v in dataset.items():\n",
    "        gutils.download(root_url + k, os.path.join(data_dir, k), sha1_hash=v)\n",
    "        \n",
    "        \n",
    "def load_data_pikachu(batch_size, edge_size=256):  # edge_size：输出图像的宽和高\n",
    "    data_dir = 'data/pikachu'\n",
    "    _download_pikachu(data_dir)\n",
    "    train_iter = image.ImageDetIter(\n",
    "        path_imgrec=os.path.join(data_dir, 'train.rec'),\n",
    "        path_imgidx=os.path.join(data_dir, 'train.idx'),\n",
    "        batch_size=batch_size,\n",
    "        data_shape=(3, edge_size, edge_size),  # 输出图像的形状\n",
    "        shuffle=True,  # 以随机顺序读取数据集\n",
    "        rand_crop=1,  # 随机裁剪的概率为1\n",
    "        min_object_covered=0.95, max_attempts=200)\n",
    "    val_iter = image.ImageDetIter(\n",
    "        path_imgrec=os.path.join(data_dir, 'val.rec'), batch_size=batch_size,\n",
    "        data_shape=(3, edge_size, edge_size), shuffle=False)\n",
    "    return train_iter, val_iter\n",
    "\n",
    "batch_size = 8\n",
    "train_iter, _ = load_data_pikachu(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义loss和metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "bbox_loss = gloss.L1Loss()\n",
    "\n",
    "def calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks):\n",
    "    cls = cls_loss(cls_preds, cls_labels)\n",
    "    bbox = bbox_loss(bbox_preds * bbox_masks, bbox_labels * bbox_masks)\n",
    "    return cls + bbox\n",
    "\n",
    "\n",
    "\n",
    "def cls_eval(cls_preds, cls_labels):\n",
    "    # 由于类别预测结果放在最后一维，argmax需要指定最后一维\n",
    "    return (cls_preds.argmax(axis=-1) == cls_labels).sum().asscalar()\n",
    "\n",
    "def bbox_eval(bbox_preds, bbox_labels, bbox_masks):\n",
    "    return ((bbox_labels - bbox_preds) * bbox_masks).abs().sum().asscalar()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "ctx, net = mx.gpu(), TinySSD(num_classes=1)\n",
    "net.initialize(init=init.Xavier(), ctx=ctx)\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd',\n",
    "                        {'learning_rate': 0.2, 'wd': 5e-4})\n",
    "\n",
    "\n",
    "for epoch in range(20):\n",
    "    acc_sum, mae_sum, n, m = 0.0, 0.0, 0, 0\n",
    "    train_iter.reset()  # 从头读取数据\n",
    "    start = time.time()\n",
    "    for batch in train_iter:\n",
    "        X = batch.data[0].as_in_context(ctx)\n",
    "        Y = batch.label[0].as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            # 生成多尺度的锚框，为每个锚框预测类别和偏移量\n",
    "            anchors, cls_preds, bbox_preds = net(X)\n",
    "            # 为每个锚框标注类别和偏移量\n",
    "            bbox_labels, bbox_masks, cls_labels = contrib.nd.MultiBoxTarget(\n",
    "                anchors, Y, cls_preds.transpose((0, 2, 1)))\n",
    "            # 根据类别和偏移量的预测和标注值计算损失函数\n",
    "            l = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,\n",
    "                          bbox_masks)\n",
    "        l.backward()\n",
    "        trainer.step(batch_size)\n",
    "        acc_sum += cls_eval(cls_preds, cls_labels)\n",
    "        n += cls_labels.size\n",
    "        mae_sum += bbox_eval(bbox_preds, bbox_labels, bbox_masks)\n",
    "        m += bbox_labels.size\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print('epoch %2d, class err %.2e, bbox mae %.2e, time %.1f sec' % (\n",
    "            epoch + 1, 1 - acc_sum / n, mae_sum / m, time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.imread('../img/pikachu.jpg')\n",
    "feature = image.imresize(img, 256, 256).astype('float32')\n",
    "X = feature.transpose((2, 0, 1)).expand_dims(axis=0)\n",
    "\n",
    "\n",
    "def predict(X):\n",
    "    anchors, cls_preds, bbox_preds = net(X.as_in_context(ctx))\n",
    "    cls_probs = cls_preds.softmax().transpose((0, 2, 1))\n",
    "    output = contrib.nd.MultiBoxDetection(cls_probs, bbox_preds, anchors)\n",
    "    idx = [i for i, row in enumerate(output[0]) if row[0].asscalar() != -1]\n",
    "    return output[0, idx]\n",
    "\n",
    "output = predict(X)\n",
    "\n",
    "\n",
    "d2l.set_figsize((5, 5))\n",
    "\n",
    "def display(img, output, threshold):\n",
    "    fig = d2l.plt.imshow(img.asnumpy())\n",
    "    for row in output:\n",
    "        score = row[1].asscalar()\n",
    "        if score < threshold:\n",
    "            continue\n",
    "        h, w = img.shape[0:2]\n",
    "        bbox = [row[2:6] * nd.array((w, h, w, h), ctx=row.context)]\n",
    "        d2l.show_bboxes(fig.axes, bbox, '%.2f' % score, 'w')\n",
    "\n",
    "display(img, output, threshold=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
